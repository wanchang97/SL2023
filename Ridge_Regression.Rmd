---
title: 'Assignment 1: Ridge Regression'
subtitle: Wanchang Zhang, Lavinia Hriscu, Victor Jimenez
output:
  pdf_document:
    toc: yes
  html_document:
    fig_caption: yes
    toc: no
    number_sections: no
    toc_float:
      collapsed: yes
    code_folding: hide
    theme: spacelab
    highlight: tango
    fig_width: 10
    fig_weight: 10
  word_document:
    toc: no
---

******

```{r, include=FALSE, echo=FALSE} 
library(Hmisc)
```

In this assignment we will fit our data to a multiple linear regression model by means of a ridge regression. Considering $n$ pairs of observations $(x_i, y_i)$, where $x_i \in 	\mathbb{R}^p$ and $y_i \in \mathbb{R}$, the linear model is the following:

$$ y_i = \beta_0 + \sum_{j=1}^p x_{ij} \beta_j + \epsilon_i $$

where $\epsilon_i$ account for white noise and $\beta \in \mathbb{R}^{p+1}$ is the vector of coefficients of the model. By fitting the model, estimators for $\beta$ and $\sigma^2$ will be obtained and the object variables $y$ will be predicted from explanatory variables $x$ as $\hat{y}_i = x_i^T \hat{\beta}$.

In general (OLS), the estimation of the coefficients is achieved by minimizing the square of the prediction error $\epsilon$. It can be proven (Gauss-Markov) that this method provides the MVUE estimator of $\beta$. In the case of ridge regression, an extra penalization term is added in order to grant the numerical stability of the method by imposing a shrinkage of the parameter vector $\hat{\beta}$. For the cases where $p >n$, for example, the penalization term allows the minimization problem to reach an optimum value and provide the "shortest' vector possible. The estimation of the coefficients $\hat{\beta}_{ridge}$ is obtained as follows:

$$ \hat{\beta}_{ridge} = \arg \min_{\beta} \left \{ \sum_{i=1}^n \left ( y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j \right )^2 + \lambda \sum_{j=1}^p \beta_j ^ 2  \right \}$$

The greater the value of the penalization parameter $\lambda \geq 0$, the greater the amount of shrinkage of the estimator $\hat{\beta}_{ridge}$. If we express it in matrix notation is easy to see that the problem has an explicit solution:

$$ \hat{y}_{ridge} = X^T \hat{\beta}_{ridge} = X^T \left ( X^T X + \lambda I_p \right )^{-1} X^T y = H_{\lambda} y $$
where $H_{\lambda}$ is the hat matrix for ridge regression.

The main goal of this assignment is to implement different methods for the choice of $\lambda$, as lower values will yield a more complex model and viceversa. The complexity of various models can be compared with the number of degrees of freedom of the model, that in the case of ridge regression depends on $\lambda$ according to the following expression:

$$ df(\lambda) = Trace(H_{\lambda}) = \sum_{j=1}^p \frac{\Lambda_j(X)^2}{\Lambda_j(X)^2 + \lambda}  $$
where $\Lambda(X)$ is the set of eigenvalues of the matrix $X_T$ of the training set $\mathcal{Z}$. The function that provides $df(\lambda)$ is the following:

```{r}
df.fun = function(X, lambda.v){
  n.lambdas = length(lambda.v)
  XtX <- t(X)%*%X # X'X
  XtX.vaps <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values
  df.lambda = matrix(0,nrow=n.lambdas, ncol=1)
  for (l in 1:n.lambdas){ 
    lambda = lambda.v[l]
    df.lambda[l,] = sum(XtX.vaps/(XtX.vaps+lambda))
  } 
  return(df.lambda)
}
```

# 1. Choosing the penalization parameter $\lambda$

As follows from the previous development, the $\lambda$ chosen will be that which minimizes the predictive error of the model, i.e. the mean square error committed when providing an estimation of new observations. That can be defined as:

$$ PMSE(\hat{\beta}_{ridge}) = \mathbb{E}_{\mathcal{Z}, X_{V}} \left [ (Y_V - X_{V} \hat{\beta}_{ridge})^2 \right ] \approx \frac{1}{n_V} \sum_{i=1}^{n_V} \left ( y_{V_i} - x_{V_i}^T \hat{\beta}_{ridge} \right )^2$$
where $X_{V}$ is the matrix of observations from the validation set $\mathcal{X}$, which is independent from the set of observations $\mathcal{Z}$ that have been used to fit the model, known as training set.

## 1.1 $\lambda$ choice with a validation set

In this first case our data will be divided into a training set $\mathcal{Z}$ and a validation set $\mathcal{X}$ according to the binary variable `train` of the dataset.

```{r}
#setwd("...")
prostate <- read.table("prostate_data.txt", header=TRUE, row.names = 1)

# Generate vector of lambdas
lambda.v = exp(seq(0,log(1e5+1),length=25))-1 
n.lambdas = length(lambda.v)

# Divide into training and validation sets
training.sample = which(prostate$train==TRUE)
validation.sample = which(prostate$train==FALSE) 

# Obtain observation matrices. X scaled and Y centered.
Y = scale(prostate$lpsa[training.sample], center=TRUE, scale=FALSE)
X = scale( as.matrix(prostate[training.sample,1:8]), center=TRUE, scale=TRUE)
Yv = scale(prostate$lpsa[validation.sample], center=TRUE, scale=FALSE)
Xv = scale( as.matrix(prostate[validation.sample,1:8]), center=TRUE, scale=TRUE)
```

The function that computes the predictive error $PMSE(\lambda)$ between the predicted $X_V \beta$ and real $Y_v$ target values of the validation set $\mathcal{X}$ is the following. The error is computed by fitting observations in the training set $\mathcal{Z}$ into the model for each value of $\lambda$.

```{r}
pmse_val <- function(X, Y, Xv, Yv, lambda.v) {
  nv <- dim(Xv)[1]
  p <- dim(X)[2]

  pmse.lambda = matrix(0,nrow=length(lambda.v), ncol=1)
  for (l in 1:length(lambda.v)){ 
    beta.lambda = t(solve(t(X)%*%X + lambda.v[l]*diag(1,p))) %*% t(X) %*% Y # coefficients
    error.lambda = Yv - Xv %*% beta.lambda  # error
    pmse.lambda[l,] = (1/nv) * t(error.lambda) %*% error.lambda  # pmse
  } 
  return(pmse.lambda)
}
```

We can plot $PMSE(\lambda)$ vs $df(\lambda)$ and identify the penalization coefficient that minimizes the error by calling our `plot.pmse.df` function. When `bool_log` is `TRUE`, then the PMSE is presented against $log(1+\lambda)-1)$ instead.

```{r, echo=FALSE}
plot.pmse.df = function(df.lambda, pmse.lambda, title, bool_log){
  if (bool_log == TRUE){
    df.lambda = log(1+lambda.v)-1
  }
  plot(range(df.lambda), range(pmse.lambda),type="n",xlab="df(lambda)",
       ylab="PMSE(lambda)", main=title)
  lines(df.lambda, pmse.lambda, col=4)
  points(df.lambda, pmse.lambda, pch=19,cex=1,col=4)
  posmin = which.min(pmse.lambda)
  points(df.lambda[posmin], pmse.lambda[posmin], pch=19,cex=1,col=2)
  abline(v=df.lambda[posmin],col=4,lty=3,lwd=2)
  lambda.text = paste("l = ", round(lambda.v[posmin],2),sep="")
  legend('topright', legend=c(lambda.text), 
         pch = c(16), col = c(2))
}
```

The implementation of `pmse_val` function into the `prostate` database yields the following result:

```{r, echo=FALSE, out.width="70%", fig.align = 'center'}
df.lambda = df.fun(X, lambda.v)
pmse.lambda = pmse_val(X, Y, Xv, Yv, lambda.v)
plot.pmse.df(df.lambda, pmse.lambda, "PMSE(lambda) in validation set", FALSE)
```


## 1.2 $\lambda$ choice with k-fold cross-validation

In some cases where the number of observations is small or in large scale data applications it is required that the whole dataset is used to fit the model (i.e. no validation set is obtained). In these cases, the computation of PMSE can be performed by cross-validation, by randomly dividing the data in $k$ sets and fitting the model $k$ times, each time leaving the $i$-th set out as validation set. This requires fitting the model $k$ times for each $\lambda$.

```{r}
pmse_kCV = function(X, Y, lambda.v, k){
  n.lambdas = length(lambda.v)
  p = dim(X)[2]

  set.seed(123) # for reproducibility
  folds = sample(rep(1:k, length.out = dim(X)[1])) # split in k sets
  pmse.lambda = matrix(0,nrow=n.lambdas, ncol=1)
  for (i in 1:k) {
    # Get the training and validation subsets
    x_train <- X[folds != i,]
    y_train <- Y[folds != i]
    x_val <- X[folds == i,]
    y_val <- Y[folds == i]
    nv = nrow(x_val)

    # Now we fit the model for each lambda
    for (l in 1:n.lambdas){ 
      beta.lambda = t(solve(t(x_train)%*%x_train + lambda.v[l]*diag(1,p))) %*% t(x_train) %*% y_train
      error.lambda = y_val - x_val %*% beta.lambda  # error
      pmse.lambda[l] = pmse.lambda[l] + t(error.lambda)%*%error.lambda  # pmse
    } 
  }
  pmse.cv = pmse.lambda/dim(X)[1]  # divide by n
  return(pmse.cv)
}
```


On `prostate` database:
```{r, echo=FALSE,out.width="70%",fig.align = 'center'}
k = 5
pmse.cv = pmse_kCV(X, Y, lambda.v, k)

# Plot
title = paste('Evolution of PMSE as a function of df [', k, sep="")
title = paste(title, '-fold CV]', sep='')
plot.pmse.df(df.lambda, pmse.cv, title, FALSE)
```

## 1.3 Implementation of the routines to `prostate` dataset

We will implement the previous routines choose $\lambda$ according to the behaviour of $PMSE(\lambda)$ in the validation set and also in 5-fold and 10-fold CV. Now the vector of $\lambda$ candidates will be of length 100, so that more accurate results can be obtained. These results will be compared with those obtained with LOO-CV and GCV.

```{r, echo=FALSE}
# Let's train the regression to find the optimal lambda.
lambda.v = exp(seq(0,log(1e5+1),length=100))-1 
n.lambdas = length(lambda.v)
df.lambda = df.fun(X, lambda.v) # recalculate degrees of freedom
  
# (1) Validation 
pmse.val = pmse_val(X, Y, Xv, Yv, lambda.v)
posmin = which.min(pmse.val)
lambda.val = lambda.v[posmin]
df.lambda.val = df.lambda[posmin]
# (2) 5-fold cross-validation
pmse.5 = pmse_kCV(X, Y, lambda.v, 5)
posmin = which.min(pmse.5)
lambda.5 = lambda.v[posmin]
df.lambda.5 = df.lambda[posmin]
# (2) 10-fold cross-validation
pmse.10 = pmse_kCV(X, Y, lambda.v, 10)
posmin = which.min(pmse.10)
lambda.10 = lambda.v[posmin]
df.lambda.10 = df.lambda[posmin]

# We can compare with leave-one-out CV easily with our function taking k=n
pmse.loo = pmse_kCV(X, Y, lambda.v, dim(X)[1])
posmin = which.min(pmse.loo)
lambda.loo = lambda.v[posmin]
df.lambda.loo = df.lambda[posmin]

# And compute generalized-cross-validation as follows:
n = dim(X)[1]
p = dim(X)[2]
pmse.gcv = numeric(n.lambdas)
for (l in 1:n.lambdas){
  lambda = lambda.v[l]
  H.lambda = X %*% t(solve(t(X)%*%X + lambda*diag(1,p))) %*% t(X) # X (X'X + lId)^{-1} X'
  nu = sum(diag(H.lambda))
  pmse.gcv[l] = sum(((Y-H.lambda%*% Y)/(1-nu/n))^2 )/n
}
posmin = which.min(pmse.gcv)
lambda.gcv = lambda.v[posmin]
df.lambda.gcv = df.lambda[posmin]


# NOW WE CAN COMPARE THE RESULTS:
torange = c(pmse.val, pmse.5, pmse.10, pmse.loo, pmse.gcv)
plot(range(df.lambda), c(max(torange),min(torange)),type="n",xlab="df(lambda)",ylab="PMSE(lambda)",
     main='lambda choice for different methods')

lines(df.lambda, pmse.val, col=4)
points(df.lambda, pmse.val, pch=19,cex=.7,col=4)
abline(v=df.lambda.val,col=4,lty=3,lwd=2)

lines(df.lambda, pmse.5, col=1)
points(df.lambda, pmse.5, pch=19,cex=.7,col=1)
abline(v=df.lambda.5,col=1,lty=3,lwd=2)

lines(df.lambda, pmse.10, col=3)
points(df.lambda, pmse.10, pch=19,cex=.7,col=3)
abline(v=df.lambda.10,col=3,lty=3,lwd=2)

lines(df.lambda, pmse.loo, col=6)
points(df.lambda, pmse.loo, pch=19,cex=.7,col=6)
abline(v=df.lambda.loo,col=6,lty=3,lwd=2)

lines(df.lambda, pmse.gcv, col=7)
points(df.lambda, pmse.gcv, pch=19,cex=.7,col=7)
abline(v=df.lambda.gcv,col=7,lty=3,lwd=2)

legend.text = c(paste("validation set => l=", round(lambda.val,2)),
                paste("5-fold CV => l=", round(lambda.5,2)),
                paste("10-fold CV => l=", round(lambda.10,2)),
                paste("LOO-CV => l=", round(lambda.loo,2)),
                paste("GCV => l=", round(lambda.gcv,2)))

legend('topright', legend=legend.text, 
       pch = c(16,16,16,16,16), col = c(4,1,3,6,7))
```



# 2. Ridge regression for `boston` housing data


The functions used in the first exercise (validation set, LOO cross-validation, K-fold cross validation) are now applied to the Boston dataset provided. 

```{r}
load("boston.Rdata")

# Select only the variables of interest to the model, in the order desired:
boston = boston.c[c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS",
                  "RAD","TAX","PTRATIO","B","LSTAT","MEDV")]
nobs = dim(boston)[1] # 506 observations

# Build X and Y matrix:
Y = scale(boston$MEDV, center=TRUE, scale=FALSE)

# Since CHAS is a binary variable, we don't have to include it in the scale. 
X1 = scale(boston[,c(-4,-14)], center=TRUE, scale=TRUE)
X = cbind(X1, as.numeric(boston$CHAS)-1) #Include it again 
colnames(X) = c(colnames(X1), "CHAS")
p = dim(X)[2] # 13 variables
```


The response variable of this dataset is MEDV. Notice that since CHAS is a factor (binary) variable, it has not been taken into account in the scale of the dataset, it has been removed and added again after the scaling of all the other explanatory variables. For the $\lambda$ tuning using a validation set, $30\%$ of the observations were kept apart.

```{r}
ntrain <- round(0.7 * nobs)
indtrain <- sample(1:nobs, ntrain, replace = FALSE)
Xtrain <- X[indtrain, ]
Ytrain <- Y[indtrain,]
Xval <- X[-indtrain,]
Yval <- Y[-indtrain,]
```

The rest of the procedure is equivalent to that presented in previous sections. The representation of $PMSE(\lambda)$ and its associated $\lambda_{min}$ for the different methods is presented below.

```{r, echo=FALSE}
df.lambda = df.fun(X, lambda.v) # recalculate degrees of freedom

pmse_val <- pmse_val(Xtrain,Ytrain, Xval, Yval, lambda.v)
posmin = which.min(pmse_val)
lambda.valid = lambda.v[posmin]
df.lambda.valid = df.lambda[posmin]

# (1) LOO cross-validation
pmse.loo = pmse_kCV(X, Y, lambda.v, nobs)
posmin = which.min(pmse.loo)
lambda.loo = lambda.v[posmin]
df.lambda.loo = df.lambda[posmin]

# (2) 5-fold cross-validation
pmse.5 = pmse_kCV(X, Y, lambda.v, 5)
posmin = which.min(pmse.5)
lambda.5 = lambda.v[posmin]
df.lambda.5 = df.lambda[posmin]

# (3) 10-fold cross-validation
pmse.10 = pmse_kCV(X, Y, lambda.v, 10)
posmin = which.min(pmse.10)
lambda.10 = lambda.v[posmin]
df.lambda.10 = df.lambda[posmin]

# We plot to compare the results:
torange = c(pmse.5, pmse.10, pmse.loo)
plot(range(df.lambda), c(max(torange),min(torange)),type="n",xlab="df(lambda)",ylab="PMSE(lambda)",
     main='lambda choice for different methods')


lines(df.lambda, pmse_val, col=4)
points(df.lambda, pmse_val, pch=19,cex=.7,col=4)
abline(v=df.lambda.valid ,col=4,lty=3,lwd=2)


lines(df.lambda, pmse.5, col=1)
points(df.lambda, pmse.5, pch=19,cex=.7,col=1)
abline(v=df.lambda.5,col=1,lty=3,lwd=2)

lines(df.lambda, pmse.10, col=3)
points(df.lambda, pmse.10, pch=19,cex=.7,col=3)
abline(v=df.lambda.10,col=3,lty=3,lwd=2)

lines(df.lambda, pmse.loo, col=6)
points(df.lambda, pmse.loo, pch=19,cex=.7,col=6)
abline(v=df.lambda.loo,col=6,lty=3,lwd=2)

legend.text = c(paste("validation set => l=", round(lambda.valid,2)),
                paste("5-fold CV => l=", round(lambda.5,2)),
                paste("10-fold CV => l=", round(lambda.10,2)),
                paste("LOO-CV => l=", round(lambda.loo,2))
                )


legend('top', legend=legend.text, 
       pch = c(16,16,16,16), col = c(4,1,3,6))
```

From the plots, we observe that the lambda corresponding to the minimum error is for all of them around 12 degrees of freedom, therefore it's reasonable to approximate the values of $\beta$ and $\lambda$ for $df=12$. The corresponding value of $\lambda$ is:

```{r}
lambda.12 <- approx(x=df.lambda, y=lambda.v, xout=12)$y
lambda.12
```

If we compute the coefficients using ridge regression we obtain:

```{r}
beta.12 = t(solve(t(X)%*%X + lambda*diag(1,p))) %*% t(X) %*% Y
beta.12.sorted = sort(beta.12, decreasing = TRUE)
beta.12
```

Focusing on the factors with higher (absolute) value, which are RM, INDUS, PTRATIO and LSTAT:
  
* LSTAT: it's negative and it measures the % of lower status of the population. If the value of LSTAT is high, it means the purchasing power is lower therefore the home price cost has to be smaller.
* RM: it's positive, this makes sense since it is the average number of rooms per dwelling, the higher the number, the more likely it is that the home cost is higher.
* PTRATIO: it's negative, this means that the higher the cost of the house, the less number of pupils per teacher, which is kind of a privilege.
* INDUS: it is referred to the number of non-retail business acres per town. If town has a large amount of non-retail areas, it's likely that the price cost is lower since the landscape of factories is not beautiful.

Another thing to notice is that there are some variables, as $B$, that at first attempt might be considered to be
relevant in Boston, but it's not. We have found that the number of black people in Boston is around 30%, therefore that might be the reason why the value $\beta$ associated to $B$ is small thus not very significant.













