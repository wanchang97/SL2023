---
title: "Decision Trees Lab."
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes

bibliography: scholar.bib
---


\newpage


```{r setup_rmd, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("knitr")) install.packages("knitr")
if(!require("tree")) install.packages("tree")
if(!require("ISLR")) install.packages("ISLR")


require("knitr")
require("tree")
require("ISLR")


```


```{r assignments}
# In this chunk, you do the assignment of values to R variables
# This code must be adapted to any change of values of R variables

myDescription <- "The data are a simulated data set containing sales of child car seats
at different stores [@james2013introduction]"
mydataset <- Carseats
```


```{r, dataDescription}
n <- nrow(mydataset)
p <- ncol(mydataset)
```

# Description data

***
_In this section,  you should be do a short explain about problem and the data set._

***


`r myDescription`. 

The data set has **`r n`** observations on **`r p`** variables. The variable names are: *`r toString(names(mydataset))`*.

The first step is create a variable, called `High`, which takes on a value of `Yes` if the `Sales`  variable exceeds 8, and takes on a value of `No` otherwise.

```{r}
# as.factor() changes the type of variable to factor
mydataset$High=as.factor(ifelse(mydataset$Sales<=8,"No","Yes"))
```

The number of observations for each class is:


```{r}
kable(table(mydataset$High), caption= "Number of observations for each class",
      col.names = c('High','Freq'))
```


The aim is of this study is predict the `High`  using all variables but `Sales` so it is a classification problem.

In this case, it is apply the classification tree model.

This is a short  data set summary

```{r}
summary(mydataset)
```



# Preprocess

***
_Sometimes it is  should be do a data preprocess before to train the model_

***

In this model is not need a data preprocess. 


# Partition of data

In order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error. We split the observations into a training set and a test set, build the model using the training set, and evaluate its performance on the test data.


```{r, dataPartition}
set.seed(2)
pt <- 1/2
train <- sample(1:nrow(mydataset),pt*nrow(mydataset))
mydataset.test <- mydataset[-train,]
High.test <-  mydataset[-train,"High"]
```

The train set has `r length(train)` observations and the test set has `r nrow(mydataset) - length(train)`.

In train data, the number of observations for each class is:

```{r}
kable(table(mydataset[train,"High"]), caption= "Train data: number of observations for each class",
      col.names = c('High','Freq'))
```

# Train model 

We now use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales` using only de train set.

```{r}
tree.mydataset=tree(High~.-Sales, mydataset, subset=train, split="deviance")
```

The `summary()` function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the **training** error rate

```{r}
summary(tree.mydataset)
```

For a classification reported in the output of `summary()` is given by

$$
-2 \sum_m \sum_k n_{mk} log(\hat{p}_{mk}),
$$

where $n_{mk}$ is the number of observations in the `m`th terminal node that belong to the `k`th class. The *residual mean difference* reported is simply the deviance divided by $n - |T_0|$ where $T_0$ is the number of terminal nodes. 


The next step is display the tree graphically. We use the `plot()` function to display the tree structure, and the `text()`function to display the node labels. 

```{r fig.DT1, fig.cap="Classification tree", fig.height=10, fig.width=12}
plot(tree.mydataset)
text(tree.mydataset,pretty=0)
```

It is also possible to show a `R` prints output corresponding to each branch of the tree. 

```{r}
tree.mydataset
```


# Prediction

We now evaluate the performance of the classification tree on the test data. The `predict()` function can be used for this purpose.


```{r}
tree.pred=predict(tree.mydataset,mydataset.test,type="class")
res <- table(tree.pred,High.test)
res
accrcy <- sum(diag(res)/sum(res))
```


The accuracy is **`r accrcy`** or misclassification error rate is **`r 1-accrcy`**.



# Prune the decision tree (Tunning model) with prediction

We consider whether pruning the tree lead to improved results. 

```{r}
set.seed(3)
cv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)
names(cv.mydataset)
cv.mydataset
```


Note that, despiste the name, `dev` corresponds to the cross-validation error rate in this instance. 

We plot the error rate as a function of both `size`and `k`.

```{r}
par(mfrow=c(1,2))
plot(cv.mydataset$size,cv.mydataset$dev,type="b")
plot(cv.mydataset$k,cv.mydataset$dev,type="b")
par(mfrow=c(1,1))
```

We now apply the `prune.misclass()` function in order to prune the tree to obtain the a best tree. The best tree is the tree with ...

```{r fig.DT2, fig.cap="The best classification pruned tree", fig.height=10, fig.width=12}
prune.mydataset=prune.misclass(tree.mydataset,
                               best=cv.mydataset$size[which.min(cv.mydataset$dev)])
plot(prune.mydataset)
text(prune.mydataset,pretty=0)

```

How well does this pruned tree perform on the test data set?

```{r}
tree.pred=predict(prune.mydataset,mydataset.test,type="class")
res <- table(tree.pred,High.test)
res
accrcy <- sum(diag(res)/sum(res))
```

The accuracy is **`r accrcy`**.

If we increase the value of `best`, for example `r  cv.mydataset$size[1]` terminal nodes, we obtain a larger pruned tree with lower classification accuracy:

```{r fig.DT3, fig.cap="Other classification pruned tree", fig.height=10, fig.width=12}
prune.mydataset=prune.misclass(tree.mydataset, 
                               best = cv.mydataset$size[1])
plot(prune.mydataset)
text(prune.mydataset, pretty=0)
```

```{r}
tree.pred=predict(prune.mydataset, mydataset.test, type="class")
res <- table(tree.pred, High.test)
res
accrcy <- sum(diag(res)/sum(res))
```

The accuracy is **`r accrcy`**.


# References

